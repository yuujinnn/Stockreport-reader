# -*- coding: utf-8 -*-
"""
FastAPI Upload Service for PDF Files
Handles file uploads and automatically processes them with RAG pipeline
"""

import os
import io
import json
import uuid
import subprocess
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any

from fastapi import FastAPI, UploadFile, File, HTTPException, status, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import pymupdf  # fitzë¥¼ ëŒ€ì‹ í•´ì„œ pymupdf ì‚¬ìš©

# uvicornê³¼ í˜¸í™˜ë˜ëŠ” ë¡œê¹… ì„¤ì •
logger = logging.getLogger("uvicorn.error")  # uvicornì˜ ê¸°ë³¸ ë¡œê±° ì‚¬ìš©
logger.setLevel(logging.INFO)

# Load environment variables from secrets
backend_root = Path(__file__).parent
secrets_path = backend_root / "secrets" / ".env"
load_dotenv(secrets_path)

# Configuration - RAG êµ¬ì¡°ì— ë§ê²Œ í†µì¼

# í˜„ì¬ ìœ„ì¹˜ì—ì„œ rag ë””ë ‰í† ë¦¬ ì°¾ê¸° (backendì—ì„œ ì‹¤í–‰ vs í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰)
current_dir = Path.cwd()
logger.info(f"ğŸ”§ Current working directory: {current_dir}")

if (current_dir / "rag").exists():
    # backend ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
    RAG_BASE_DIR = Path("rag")
    logger.info(f"âœ… Found rag directory in current path: {RAG_BASE_DIR.resolve()}")
elif (current_dir / "backend" / "rag").exists():
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
    RAG_BASE_DIR = Path("backend") / "rag"
    logger.info(f"âœ… Found rag directory in backend path: {RAG_BASE_DIR.resolve()}")
else:
    # ê¸°ë³¸ê°’
    RAG_BASE_DIR = Path("rag")
    logger.warning(f"âš ï¸ Using default rag path: {RAG_BASE_DIR.resolve()}")

UPLOAD_DIR = RAG_BASE_DIR / "data" / "pdf"  # RAG PDF ë””ë ‰í† ë¦¬ì™€ í†µì¼
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

logger.info(f"ğŸ“ RAG_BASE_DIR: {RAG_BASE_DIR.resolve()}")
logger.info(f"ğŸ“ UPLOAD_DIR: {UPLOAD_DIR.resolve()}")
logger.info(f"ğŸ“„ processed_states.json path: {(RAG_BASE_DIR / 'data' / 'vectordb' / 'processed_states.json').resolve()}")

# Ensure directories exist
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# FastAPI app
app = FastAPI(
    title="Stockreport PDF Upload Service",
    description="Handles PDF uploads and automatically processes them with RAG pipeline",
    version="2.0.0"
)

# ì•± ë¡œë”© í™•ì¸ìš© ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/test")
async def test_endpoint():
    """ì•±ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ìš© í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    print("ğŸ§ª TEST ENDPOINT CALLED!")
    return {"status": "working", "message": "FastAPI app is loaded correctly"}

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸"""
    print("ğŸš€ Starting Stockreport PDF Upload Service")
    print("ğŸŒ Available endpoints:")
    print("  â€¢ Test: http://localhost:9000/test")
    print("  â€¢ Root info: http://localhost:9000/")
    print("  â€¢ Upload PDF: POST http://localhost:9000/upload")
    print("  â€¢ Existing files: GET http://localhost:9000/files")
    print("  â€¢ Debug files: http://localhost:9000/debug/files")
    print("  â€¢ Debug uploads: http://localhost:9000/debug/uploads")
    print("  â€¢ Chunk data: http://localhost:9000/chunks/{file_id}")
    print("  â€¢ Health check: http://localhost:9000/health")
    print("ğŸ” Debugging steps:")
    print("  1. First check: http://localhost:9000/test")
    print("  2. Then check: http://localhost:9000/debug/files")
    print("  3. Get file_id: http://localhost:9000/debug/uploads")
    
    # ê¸°ì¡´ PDF íŒŒì¼ë“¤ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ìë™ ìƒì„±
    auto_generate_metadata_for_existing_pdfs()
    
    # ê¸°ì¡´ íŒŒì¼ ìš”ì•½ ì¶œë ¥
    summary = get_existing_files_summary()
    print(f"\nğŸ“Š File Summary:")
    print(f"  â€¢ PDF files: {len(summary['pdf_files'])}")
    print(f"  â€¢ Metadata files: {len(summary['metadata_files'])}")
    print(f"  â€¢ Processed files: {len(summary['processed_files'])}")
    
    if summary['processed_files']:
        print(f"  â€¢ Ready to use: {summary['processed_files']}")
    
    # FastAPI ì•±ì˜ ë“±ë¡ëœ ì—”ë“œí¬ì¸íŠ¸ë“¤ í™•ì¸
    print("\nğŸ“‹ Registered FastAPI routes:")
    for route in app.routes:
        if hasattr(route, 'methods') and hasattr(route, 'path'):
            print(f"  â€¢ {', '.join(route.methods)} {route.path}")
    
    # Environment variables check
    required_vars = ["UPSTAGE_API_KEY", "OPENAI_API_KEY", "CLOVASTUDIO_API_KEY"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"âš ï¸ Missing environment variables: {missing_vars}")
    else:
        print("âœ… All required environment variables found")


# Models
class UploadResponse(BaseModel):
    fileId: str
    pages: Optional[int] = None
    filename: str
    uploadedAt: str
    processingStatus: str = "queued"  # queued, processing, completed, failed


class ChunkInfo(BaseModel):
    chunk_id: str
    page: int
    bbox_norm: List[float]  # [left, top, right, bottom] normalized 0-1
    chunk_type: str  # "text", "image", "table"
    content: str  # í…ìŠ¤íŠ¸ ë‚´ìš© ë˜ëŠ” ìš”ì•½
    label: Optional[str] = None
    # í˜ì´ì§€ í¬ê¸° ì •ë³´ ì¶”ê°€ (RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•œ ì‹¤ì œ í¬ê¸°)
    page_width: Optional[float] = None
    page_height: Optional[float] = None
    # ì›ë³¸ í”½ì…€ ì¢Œí‘œ ì¶”ê°€ (ë””ë²„ê¹… ë° ì¬ê³„ì‚°ìš©)
    bbox_pixels: Optional[List[int]] = None  # [left, top, right, bottom] in pixels


class FileMetadata(BaseModel):
    file_id: str
    original_filename: str
    saved_filename: str  # RAGì—ì„œ ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ íŒŒì¼ëª…
    page_count: int
    upload_timestamp: str


# Helper functions
def get_pdf_page_count(file_path: Path) -> int:
    """Extract page count from PDF file using pymupdf"""
    try:
        with pymupdf.open(file_path) as doc:
            return len(doc)
    except Exception as e:
        logger.error(f"Error reading PDF: {e}")
        return 0


def save_file_metadata(metadata: FileMetadata) -> None:
    """Save simple file metadata"""
    metadata_file = UPLOAD_DIR / f"{metadata.file_id}_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata.dict(), f, ensure_ascii=False, indent=2)


def load_file_metadata(file_id: str) -> Optional[FileMetadata]:
    """Load file metadata"""
    metadata_file = UPLOAD_DIR / f"{file_id}_metadata.json"
    if not metadata_file.exists():
        return None
    
    try:
        with open(metadata_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return FileMetadata(**data)
    except Exception as e:
        logger.error(f"Error loading file metadata: {e}")
        return None


def get_rag_results(saved_filename: str) -> Optional[Dict[str, Any]]:
    """RAG ê²°ê³¼ íŒŒì¼ì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë“œ"""
    base_filename = saved_filename.replace('.pdf', '')
    rag_result_files = list(UPLOAD_DIR.glob(f"{base_filename}_*.json"))
    
    if not rag_result_files:
        return None
    
    try:
        rag_result_file = rag_result_files[0]
        with open(rag_result_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading RAG results: {e}")
        return None


def get_processed_states() -> Optional[Dict[str, Any]]:
    """processed_states.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    vectordb_dir = RAG_BASE_DIR / "data" / "vectordb"
    processed_states_file = vectordb_dir / "processed_states.json"
    
    logger.info(f"ğŸ” Looking for processed_states.json at: {processed_states_file}")
    logger.info(f"ğŸ“ Vectordb directory exists: {vectordb_dir.exists()}")
    logger.info(f"ğŸ“„ Processed states file exists: {processed_states_file.exists()}")
    
    if not processed_states_file.exists():
        logger.warning("âŒ processed_states.json file not found")
        return None
    
    try:
        with open(processed_states_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            logger.info(f"âœ… Loaded processed_states.json with {len(data)} files")
            logger.info(f"ğŸ“‹ Available files: {list(data.keys())}")
            return data
    except Exception as e:
        logger.error(f"âŒ Error loading processed states: {e}")
        return None


def normalize_bbox(bbox_points: List[Dict[str, int]], page_width: float, page_height: float) -> List[float]:
    """ë°”ìš´ë”©ë°•ìŠ¤ ì¢Œí‘œë¥¼ ì •ê·œí™” (0-1 ë²”ìœ„)"""
    try:
        # 4ê°œ ì ì—ì„œ ìµœì†Œ/ìµœëŒ€ê°’ ì¶”ì¶œ
        x_coords = [point['x'] for point in bbox_points]
        y_coords = [point['y'] for point in bbox_points]
        
        left = min(x_coords) / page_width
        right = max(x_coords) / page_width
        top = min(y_coords) / page_height
        bottom = max(y_coords) / page_height
        
        return [left, top, right, bottom]
    except Exception as e:
        logger.error(f"Error normalizing bbox: {e}")
        return [0.0, 0.0, 1.0, 1.0]


def get_pdf_page_dimensions(file_path: Path, page_num: int = 0) -> tuple[float, float]:
    """PDF í˜ì´ì§€ì˜ í¬ê¸°ë¥¼ êµ¬í•¨ (width, height) - pymupdf ì‚¬ìš©"""
    try:
        with pymupdf.open(file_path) as doc:
            if page_num < len(doc):
                page = doc[page_num]
                # pymupdfì—ì„œ get_pixmap()ì„ ì‚¬ìš©í•´ì„œ ì‹¤ì œ ë Œë”ë§ëœ í¬ê¸°ë¥¼ êµ¬í•¨
                # DPI 300ìœ¼ë¡œ ê³ ì • (RAG íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼)
                pixmap = page.get_pixmap(dpi=300)
                width = float(pixmap.width)
                height = float(pixmap.height)
                logger.info(f"ğŸ“ Page {page_num} dimensions (pymupdf): {width}x{height}")
                return width, height
    except Exception as e:
        logger.error(f"Error getting PDF page dimensions: {e}")
    
    # ê¸°ë³¸ê°’ ë°˜í™˜ (A4 í¬ê¸°, 300 DPI)
    return 2480.0, 3508.0  # A4 at 300 DPI


def get_rag_processing_status(saved_filename: str) -> str:
    """RAG ì²˜ë¦¬ ìƒíƒœ í™•ì¸"""
    # 1. PDF íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    pdf_file = UPLOAD_DIR / saved_filename
    if not pdf_file.exists():
        return "not_found"
    
    # 2. RAG ê²°ê³¼ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    base_filename = saved_filename.replace('.pdf', '')
    rag_result_files = list(UPLOAD_DIR.glob(f"{base_filename}_*.json"))
    
    if rag_result_files:
        return "completed"
    else:
        return "pending"


async def process_pdf_with_rag(file_id: str, saved_filename: str):
    """
    Background task to process PDF with RAG pipeline
    """
    try:
        logger.info(f"ğŸš€ Starting RAG processing for {saved_filename}")
        
        # Change to RAG directory for execution
        original_cwd = os.getcwd()
        os.chdir(RAG_BASE_DIR)
        
        try:
            # Execute RAG processing with the specific file
            result = subprocess.run(
                ["python", "scripts/process_pdfs.py", "--limit", "1"],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes timeout
            )
            
            if result.returncode == 0:
                logger.info(f"âœ… RAG processing completed successfully for {saved_filename}")
                
                # Check if RAG result file was created
                base_filename = saved_filename.replace('.pdf', '')
                rag_result_files = list(UPLOAD_DIR.glob(f"{base_filename}_*.json"))
                
                if rag_result_files:
                    logger.info(f"ğŸ“„ RAG results available: {[f.name for f in rag_result_files]}")
                else:
                    logger.warning(f"âš ï¸ No RAG result files found for {saved_filename}")
                
            else:
                logger.error(f"âŒ RAG processing failed for {saved_filename}")
                logger.error(f"Error output: {result.stderr}")
                    
        finally:
            # Restore original working directory
            os.chdir(original_cwd)
            
    except subprocess.TimeoutExpired:
        logger.warning(f"â° RAG processing timeout for {saved_filename}")
    except Exception as e:
        logger.error(f"ğŸ’¥ RAG processing error for {saved_filename}: {str(e)}")


def auto_generate_metadata_for_existing_pdfs():
    """
    ì„œë²„ ì‹œì‘ ì‹œ ê¸°ì¡´ PDF íŒŒì¼ë“¤ì„ ìŠ¤ìº”í•´ì„œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
    """
    logger.info("ğŸ” Scanning for existing PDF files without metadata...")
    
    if not UPLOAD_DIR.exists():
        logger.info("ğŸ“ Upload directory doesn't exist yet")
        return
    
    pdf_files = list(UPLOAD_DIR.glob("*.pdf"))
    logger.info(f"ğŸ“„ Found {len(pdf_files)} PDF files")
    
    generated_count = 0
    
    for pdf_file in pdf_files:
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        existing_metadata = list(UPLOAD_DIR.glob(f"*_{pdf_file.stem}_metadata.json"))
        
        if existing_metadata:
            logger.info(f"âœ… Metadata already exists for {pdf_file.name}")
            continue
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        try:
            page_count = get_pdf_page_count(pdf_file)
            file_id = f"{uuid.uuid4().hex}_{pdf_file.stem}"
            
            metadata = FileMetadata(
                file_id=file_id,
                original_filename=pdf_file.name,
                saved_filename=pdf_file.name,
                page_count=page_count,
                upload_timestamp=datetime.now().isoformat()
            )
            
            save_file_metadata(metadata)
            generated_count += 1
            
            logger.info(f"ğŸ“‹ Generated metadata for {pdf_file.name} with file_id: {file_id}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate metadata for {pdf_file.name}: {e}")
    
    if generated_count > 0:
        logger.info(f"ğŸ‰ Generated metadata for {generated_count} existing PDF files")
    else:
        logger.info("âœ¨ All existing PDF files already have metadata")


def get_existing_files_summary():
    """
    ê¸°ì¡´ íŒŒì¼ë“¤ì˜ ìš”ì•½ ì •ë³´ ë°˜í™˜
    """
    summary = {
        "pdf_files": [],
        "processed_files": [],
        "metadata_files": []
    }
    
    if UPLOAD_DIR.exists():
        # PDF íŒŒì¼ë“¤
        pdf_files = list(UPLOAD_DIR.glob("*.pdf"))
        summary["pdf_files"] = [f.name for f in pdf_files]
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤
        metadata_files = list(UPLOAD_DIR.glob("*_metadata.json"))
        summary["metadata_files"] = [f.name for f in metadata_files]
    
    # processed_states.jsonì—ì„œ ì²˜ë¦¬ëœ íŒŒì¼ë“¤
    processed_states = get_processed_states()
    if processed_states:
        summary["processed_files"] = list(processed_states.keys())
    
    return summary

# API Endpoints
@app.post("/upload", response_model=UploadResponse)
async def upload_pdf(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    """
    Upload a PDF file and automatically start RAG processing
    """
    # ì„ì‹œ ë””ë²„ê¹…: printì™€ logger ë‘˜ ë‹¤ ì‚¬ìš©
    print(f"ğŸ¯ UPLOAD REQUEST RECEIVED: {file.filename}")
    logger.info(f"ğŸ¯ UPLOAD REQUEST RECEIVED: {file.filename}")
    
    # Validate file type
    if not file.filename.lower().endswith('.pdf'):
        print(f"âŒ Invalid file type: {file.filename}")
        logger.error(f"âŒ Invalid file type: {file.filename}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Only PDF files are allowed"
        )
    
    # Check file size
    contents = await file.read()
    if len(contents) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail=f"File size exceeds maximum allowed size of {MAX_FILE_SIZE/1024/1024}MB"
        )
    
    # Generate unique file ID and clean filename
    clean_filename = Path(file.filename).stem
    file_id = f"{uuid.uuid4().hex}_{clean_filename}"
    
    # Save to RAG PDF directory
    file_path = UPLOAD_DIR / f"{clean_filename}.pdf"  # Use original filename for RAG compatibility
    
    # Save file
    try:
        with open(file_path, 'wb') as f:
            f.write(contents)
        print(f"ğŸ“ Saved PDF to: {file_path}")
        logger.info(f"ğŸ“ Saved PDF to: {file_path}")
    except Exception as e:
        print(f"âŒ Failed to save file: {str(e)}")
        logger.error(f"Failed to save file: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save file: {str(e)}"
        )
    
    # Get page count
    page_count = get_pdf_page_count(file_path)
    
    # Save simple metadata
    metadata = FileMetadata(
        file_id=file_id,
        original_filename=file.filename,
        saved_filename=clean_filename + ".pdf",
        page_count=page_count,
        upload_timestamp=datetime.now().isoformat()
    )
    save_file_metadata(metadata)
    
    # Add background task for RAG processing
    background_tasks.add_task(process_pdf_with_rag, file_id, clean_filename + ".pdf")
    
    print(f"ğŸ¯ Queued RAG processing for: {file.filename}")
    print(f"ğŸ“‹ File metadata saved: {file_id}_metadata.json")
    print(f"ğŸ“„ PDF saved as: {clean_filename}.pdf")
    print(f"ğŸ• Upload completed at: {datetime.now().isoformat()}")
    
    logger.info(f"ğŸ¯ Queued RAG processing for: {file.filename}")
    logger.info(f"ğŸ“‹ File metadata saved: {file_id}_metadata.json")
    logger.info(f"ğŸ“„ PDF saved as: {clean_filename}.pdf")
    logger.info(f"ğŸ• Upload completed at: {datetime.now().isoformat()}")
    
    return UploadResponse(
        fileId=file_id,
        pages=page_count if page_count > 0 else None,
        filename=file.filename,
        uploadedAt=datetime.now().isoformat(),
        processingStatus="queued"
    )


@app.get("/status/{file_id}")
async def get_processing_status(file_id: str):
    """
    Get processing status for a file
    """
    metadata = load_file_metadata(file_id)
    if not metadata:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File not found"
        )
    
    # RAG ì²˜ë¦¬ ìƒíƒœ ë° ê²°ê³¼ í™•ì¸
    rag_status = get_rag_processing_status(metadata.saved_filename)
    rag_results = get_rag_results(metadata.saved_filename) if rag_status == "completed" else None
    
    return {
        "fileId": file_id,
        "filename": metadata.original_filename,
        "pages": metadata.page_count,
        "uploadedAt": metadata.upload_timestamp,
        "ragProcessingStatus": rag_status,
        "summaryStats": {
            "textSummaries": len(rag_results.get("text_summary", {}) if rag_results else {}),
            "imageSummaries": len(rag_results.get("image_summary", {}) if rag_results else {}),
            "tableSummaries": len(rag_results.get("table_summary", {}) if rag_results else {})
        }
    }


@app.get("/summaries/{file_id}")
async def get_summaries(file_id: str):
    """
    Get RAG processing results (text, image, table summaries)
    """
    metadata = load_file_metadata(file_id)
    if not metadata:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File not found"
        )
    
    # RAG ê²°ê³¼ ì§ì ‘ ë¡œë“œ
    rag_results = get_rag_results(metadata.saved_filename)
    if not rag_results:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="RAG results not found - processing may not be completed yet"
        )
    
    return {
        "fileId": file_id,
        "filename": metadata.original_filename,
        "ragProcessingStatus": "completed",
        "summaries": {
            "textSummaries": rag_results.get("text_summary", {}),
            "imageSummaries": rag_results.get("image_summary", {}),
            "tableSummaries": rag_results.get("table_summary", {})
        },
        "stats": {
            "textCount": len(rag_results.get("text_summary", {})),
            "imageCount": len(rag_results.get("image_summary", {})),
            "tableCount": len(rag_results.get("table_summary", {}))
        }
    }


@app.get("/chunks/{file_id}", response_model=List[ChunkInfo])
async def get_chunks(file_id: str):
    """
    Get chunk information for a file from processed_states.json
    """
    logger.info(f"ğŸ” GET /chunks/{file_id} - Starting chunk retrieval")
    
    metadata = load_file_metadata(file_id)
    if not metadata:
        logger.error(f"âŒ No metadata found for file_id: {file_id}")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File not found"
        )
    
    logger.info(f"âœ… Found metadata for file: {metadata.saved_filename}")
    
    # processed_states.jsonì—ì„œ ë°ì´í„° ë¡œë“œ
    processed_states = get_processed_states()
    if not processed_states:
        logger.warning("âŒ No processed_states data available")
        return []
    
    # íŒŒì¼ëª…ìœ¼ë¡œ ë°ì´í„° ì°¾ê¸°
    saved_filename = metadata.saved_filename
    logger.info(f"ğŸ” Looking for file data: {saved_filename}")
    
    if saved_filename not in processed_states:
        logger.warning(f"âŒ File {saved_filename} not found in processed_states")
        logger.info(f"ğŸ“‹ Available files in processed_states: {list(processed_states.keys())}")
        return []
    
    logger.info(f"âœ… Found processed data for: {saved_filename}")
    
    file_data = processed_states[saved_filename]
    chunks = []
    
    # PDF íŒŒì¼ ê²½ë¡œ êµ¬ì„±
    pdf_file_path = UPLOAD_DIR / saved_filename
    
    try:
        # ê° ì²­í¬ íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
        chunk_types = [
            ("text_element_output", "text"),
            ("image_summary", "image"), 
            ("table_summary", "table")
        ]
        
        logger.info(f"ğŸ“Š Processing chunk types for {saved_filename}")
        logger.info(f"ğŸ“‹ Available sections in file_data: {list(file_data.keys())}")
        
        for section_name, chunk_type in chunk_types:
            if section_name not in file_data:
                logger.warning(f"âš ï¸ Section {section_name} not found in file data")
                continue
                
            section_data = file_data[section_name]
            logger.info(f"âœ… Processing {section_name} with {len(section_data)} chunks")
            
            for chunk_id, chunk_info in section_data.items():
                try:
                    # ë°ì´í„° íŒŒì‹±: [í˜ì´ì§€ë²ˆí˜¸, [ë°”ìš´ë”©ë°•ìŠ¤ì¢Œí‘œ], "ë‚´ìš©"]
                    page_num = chunk_info[0]
                    bbox_points = chunk_info[1]
                    content = chunk_info[2]
                    
                    # PDF í˜ì´ì§€ í¬ê¸° êµ¬í•˜ê¸° (í˜ì´ì§€ë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    page_width, page_height = get_pdf_page_dimensions(pdf_file_path, page_num)
                    
                    # ë°”ìš´ë”©ë°•ìŠ¤ ì •ê·œí™”
                    bbox_norm = normalize_bbox(bbox_points, page_width, page_height)
                    
                    # ì›ë³¸ í”½ì…€ ì¢Œí‘œ ê³„ì‚°
                    x_coords = [point['x'] for point in bbox_points]
                    y_coords = [point['y'] for point in bbox_points]
                    bbox_pixels = [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]
                    
                    # ë¼ë²¨ ìƒì„± (ì²­í¬ íƒ€ì…ì— ë”°ë¼)
                    if chunk_type == "text":
                        # í…ìŠ¤íŠ¸ì˜ ì²« 20ì ë˜ëŠ” ì²« ì¤„ì„ ë¼ë²¨ë¡œ ì‚¬ìš©
                        first_line = content.split('\n')[0]
                        label = first_line[:20] + "..." if len(first_line) > 20 else first_line
                    elif chunk_type == "image":
                        label = f"ì´ë¯¸ì§€ #{chunk_id}"
                    else:  # table
                        label = f"í…Œì´ë¸” #{chunk_id}"
                    
                    chunk = ChunkInfo(
                        chunk_id=f"{chunk_type}_{chunk_id}",
                        page=page_num + 1,  # 0-basedë¥¼ 1-basedë¡œ ë³€í™˜
                        bbox_norm=bbox_norm,
                        chunk_type=chunk_type,
                        content=content,
                        label=label,
                        page_width=page_width,
                        page_height=page_height,
                        bbox_pixels=bbox_pixels
                    )
                    chunks.append(chunk)
                    
                except (IndexError, KeyError, TypeError) as e:
                    logger.error(f"Error processing chunk {chunk_id} in {section_name}: {e}")
                    continue
    
    except Exception as e:
        logger.error(f"Error processing chunks for {saved_filename}: {e}")
        return []
    
    # í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    chunks.sort(key=lambda x: (x.page, x.chunk_id))
    
    logger.info(f"ğŸ“¦ Loaded {len(chunks)} chunks for {saved_filename}")
    return chunks


@app.get("/file/{file_id}/download")
async def download_file(file_id: str):
    """Download the uploaded PDF file"""
    import urllib.parse
    
    logger.info(f"Download request for file_id: {file_id}")
    
    # Extract filename from file_id (format: uuid_filename)
    if '_' in file_id:
        filename_part = '_'.join(file_id.split('_')[1:])
    else:
        filename_part = file_id
    
    # Try to find the file in RAG PDF directory
    file_path = None
    
    # Try exact filename match
    potential_path = UPLOAD_DIR / f"{filename_part}.pdf"
    if potential_path.exists():
        file_path = potential_path
    else:
        # Try to find any PDF file containing the filename
        for pdf_file in UPLOAD_DIR.glob("*.pdf"):
            if filename_part.lower() in pdf_file.stem.lower():
                file_path = pdf_file
                break
    
    if not file_path:
        logger.warning(f"File not found. Searched for: {filename_part}")
        available_files = [file.name for file in UPLOAD_DIR.glob("*.pdf")]
        logger.info(f"Available files: {available_files}")
            
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"File not found: {file_id}"
        )
    
    # Read file and return as streaming response
    try:
        with open(file_path, 'rb') as f:
            pdf_content = f.read()
        
        # Use the actual filename for the response
        safe_filename = urllib.parse.quote(file_path.stem)
        
        return StreamingResponse(
            io.BytesIO(pdf_content),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"inline; filename*=UTF-8''{safe_filename}.pdf",
                "Content-Length": str(len(pdf_content)),
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET",
                "Access-Control-Allow-Headers": "*"
            }
        )
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to read file: {str(e)}"
        )


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    # Check environment variables
    env_status = {
        "UPSTAGE_API_KEY": "âœ…" if os.getenv("UPSTAGE_API_KEY") else "âŒ",
        "OPENAI_API_KEY": "âœ…" if os.getenv("OPENAI_API_KEY") else "âŒ", 
        "CLOVASTUDIO_API_KEY": "âœ…" if os.getenv("CLOVASTUDIO_API_KEY") else "âŒ"
    }
    
    return {
        "status": "healthy",
        "service": "upload-api",
        "timestamp": datetime.now().isoformat(),
        "directories": {
            "upload_dir": str(UPLOAD_DIR)
        },
        "environment_variables": env_status
    }


@app.get("/")
async def root():
    """Root endpoint"""
    print("ğŸ  ROOT ENDPOINT CALLED!")
    return {
        "status": "running",
        "service": "Stockreport PDF Upload Service with RAG Integration",
        "version": "2.0.0",
        "message": "API is working correctly",
        "test_endpoint": "GET /test",
        "endpoints": {
            "upload": "POST /upload - Upload PDF and start RAG processing",
            "status": "GET /status/{file_id} - Get processing status",
            "summaries": "GET /summaries/{file_id} - Get RAG results (text/image/table summaries)",
            "chunks": "GET /chunks/{file_id} - Get chunk information",
            "download": "GET /file/{file_id}/download - Download PDF",
            "health": "GET /health - Health check",
            "debug_files": "GET /debug/files - Debug file system status",
            "debug_uploads": "GET /debug/uploads - Debug uploads status"
        },
        "features": [
            "Automatic RAG processing after upload",
            "Unified directory structure with RAG system", 
            "Background task processing",
            "Processing status tracking",
            "Chunk-based document analysis with bounding boxes"
        ]
    }


@app.get("/debug/files")
async def debug_files():
    """ë””ë²„ê¹…ìš© íŒŒì¼ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    vectordb_dir = RAG_BASE_DIR / "data" / "vectordb"
    processed_states_file = vectordb_dir / "processed_states.json"
    
    debug_info = {
        "current_working_directory": os.getcwd(),
        "rag_base_dir": str(RAG_BASE_DIR),
        "rag_base_dir_resolved": str(RAG_BASE_DIR.resolve()),
        "upload_dir": str(UPLOAD_DIR),
        "upload_dir_resolved": str(UPLOAD_DIR.resolve()),
        "vectordb_dir": str(vectordb_dir),
        "vectordb_dir_resolved": str(vectordb_dir.resolve()),
        "processed_states_file": str(processed_states_file),
        "processed_states_file_resolved": str(processed_states_file.resolve()),
        "directories": {
            "rag_base_exists": RAG_BASE_DIR.exists(),
            "upload_dir_exists": UPLOAD_DIR.exists(),
            "vectordb_dir_exists": vectordb_dir.exists(),
        },
        "files": {
            "processed_states_exists": processed_states_file.exists(),
        },
        "upload_dir_contents": [],
        "vectordb_contents": []
    }
    
    # Upload ë””ë ‰í† ë¦¬ ë‚´ìš©
    if UPLOAD_DIR.exists():
        debug_info["upload_dir_contents"] = [f.name for f in UPLOAD_DIR.iterdir()]
    
    # Vectordb ë””ë ‰í† ë¦¬ ë‚´ìš©  
    if vectordb_dir.exists():
        debug_info["vectordb_contents"] = [f.name for f in vectordb_dir.iterdir()]
    
    # processed_states.json ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
    if processed_states_file.exists():
        try:
            with open(processed_states_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                debug_info["processed_states_preview"] = {
                    "file_count": len(data),
                    "files": list(data.keys())
                }
        except Exception as e:
            debug_info["processed_states_error"] = str(e)
    
    return debug_info


@app.get("/debug/uploads")
async def debug_uploads():
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ê³¼ file_id ëª©ë¡ ì¡°íšŒ"""
    uploads_info = {
        "uploaded_files": [],
        "metadata_files": [],
        "instructions": {
            "how_to_get_file_id": "íŒŒì¼ ì—…ë¡œë“œ í›„ ì‘ë‹µì˜ 'fileId' í•„ë“œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì•„ë˜ metadata_filesì—ì„œ í™•ì¸",
            "chunk_api_format": "GET /chunks/{file_id}",
            "example": "GET /chunks/abc123_filename"
        }
    }
    
    if UPLOAD_DIR.exists():
        for file_path in UPLOAD_DIR.iterdir():
            if file_path.is_file():
                if file_path.name.endswith('_metadata.json'):
                    # ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹±
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                            uploads_info["metadata_files"].append({
                                "file_id": metadata.get("file_id"),
                                "original_filename": metadata.get("original_filename"),
                                "saved_filename": metadata.get("saved_filename"),
                                "upload_timestamp": metadata.get("upload_timestamp"),
                                "chunk_api_url": f"/chunks/{metadata.get('file_id')}"
                            })
                    except Exception as e:
                        uploads_info["metadata_files"].append({
                            "filename": file_path.name,
                            "error": str(e)
                        })
                else:
                    uploads_info["uploaded_files"].append(file_path.name)
    
    return uploads_info


@app.get("/files")
async def get_existing_files():
    """
    ê¸°ì¡´ì— ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ëª©ë¡ ì¡°íšŒ
    RAG ì²˜ë¦¬ ìƒíƒœì™€ í•¨ê»˜ ë°˜í™˜
    """
    files_info = []
    
    if not UPLOAD_DIR.exists():
        return {"files": files_info, "total": 0}
    
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ëª©ë¡ êµ¬ì„±
    metadata_files = list(UPLOAD_DIR.glob("*_metadata.json"))
    processed_states = get_processed_states()
    
    for metadata_file in metadata_files:
        try:
            metadata = load_file_metadata(metadata_file.stem.replace("_metadata", ""))
            if not metadata:
                continue
            
            # RAG ì²˜ë¦¬ ìƒíƒœ í™•ì¸
            rag_status = "not_processed"
            has_chunks = False
            
            if processed_states and metadata.saved_filename in processed_states:
                file_data = processed_states[metadata.saved_filename]
                # í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í…Œì´ë¸” ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì²˜ë¦¬ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                if (file_data.get("text_element_output") or 
                    file_data.get("image_summary") or 
                    file_data.get("table_summary")):
                    rag_status = "completed"
                    has_chunks = True
                elif file_data.get("parsing_processed"):
                    rag_status = "processing"
            
            file_info = {
                "file_id": metadata.file_id,
                "filename": metadata.original_filename,
                "saved_filename": metadata.saved_filename,
                "pages": metadata.page_count,
                "upload_timestamp": metadata.upload_timestamp,
                "rag_status": rag_status,
                "has_chunks": has_chunks,
                "download_url": f"/file/{metadata.file_id}/download",
                "chunks_url": f"/chunks/{metadata.file_id}" if has_chunks else None
            }
            
            files_info.append(file_info)
            
        except Exception as e:
            logger.error(f"Error processing metadata file {metadata_file}: {e}")
            continue
    
    # ì—…ë¡œë“œ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    files_info.sort(key=lambda x: x["upload_timestamp"], reverse=True)
    
    return {
        "files": files_info,
        "total": len(files_info),
        "summary": {
            "total_files": len(files_info),
            "rag_completed": len([f for f in files_info if f["rag_status"] == "completed"]),
            "rag_processing": len([f for f in files_info if f["rag_status"] == "processing"]),
            "not_processed": len([f for f in files_info if f["rag_status"] == "not_processed"])
        }
    }


# Note: When using uvicorn command directly, this block is not executed
# All startup logging is handled in the @app.on_event("startup") function above
if __name__ == "__main__":
    import uvicorn
    
    # This will only run when executing: python upload_api.py
    # For uvicorn command: uvicorn upload_api:app --host 0.0.0.0 --port 9000 --reload
    logger.info("ğŸ”§ Starting server via python upload_api.py")
    logger.info("ğŸ“„ Note: For production use 'uvicorn upload_api:app --host 0.0.0.0 --port 9000 --reload'")
    
    uvicorn.run(app, host="0.0.0.0", port=9000, reload=True) 